{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from importlib import import_module\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, \"../CustomizedModule\")\n",
    "from CustomizedScheduler import get_scheduler\n",
    "from CustomizedOptimizer import get_optimizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval_utils import DSTEvaluator\n",
    "from evaluation import _evaluation\n",
    "from inference import inference_TRADE\n",
    "from data_utils import train_data_loading, get_data_loader\n",
    "\n",
    "from preprocessor import TRADEPreprocessor\n",
    "from model import TRADE\n",
    "from criterions import LabelSmoothingLoss, masked_cross_entropy_for_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_informations(args):\n",
    "    # Define Tokenizer\n",
    "    tokenizer_module = getattr(\n",
    "        import_module(\"transformers\"), f\"{args.model_name}Tokenizer\"\n",
    "    )\n",
    "    tokenizer = tokenizer_module.from_pretrained(args.pretrained_name_or_path)\n",
    "\n",
    "    slot_meta, train_examples, train_labels = train_data_loading(args, isUserFirst=False, isDialogueLevel=False)\n",
    "\n",
    "    # Define Preprocessor\n",
    "    processor = TRADEPreprocessor(slot_meta, tokenizer)\n",
    "\n",
    "    # Extract Features\n",
    "    train_features = processor.convert_examples_to_features(train_examples)\n",
    "\n",
    "    # Slot Meta tokenizing for the decoder initial inputs\n",
    "    tokenized_slot_meta = []\n",
    "    for slot in slot_meta:\n",
    "        tokenized_slot_meta.append(\n",
    "            tokenizer.encode(slot.replace(\"-\", \" \"), add_special_tokens=False)\n",
    "        )\n",
    "\n",
    "    args.vocab_size = len(tokenizer)\n",
    "    args.n_gate = len(processor.gating2id)  # gating 갯수 none, dontcare, ptr\n",
    "\n",
    "    # json.dump(\n",
    "    #     vars(args),\n",
    "    #     open(f\"{args.model_dir}/{args.model_fold}/exp_config.json\", \"w\"),\n",
    "    #     indent=2,\n",
    "    #     ensure_ascii=False,\n",
    "    # )\n",
    "    # json.dump(\n",
    "    #     slot_meta,\n",
    "    #     open(f\"{args.model_dir}/{args.model_fold}/slot_meta.json\", \"w\"),\n",
    "    #     indent=2,\n",
    "    #     ensure_ascii=False,\n",
    "    # )\n",
    "    return tokenizer, processor, slot_meta, tokenized_slot_meta, train_features, train_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['관광-종류-박물관', '관광-지역-서울 중앙']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "list(train_labels.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7000/7000 [00:00<00:00, 16035.25it/s]\n",
      "[Conversion: Examples > Features]:   0%|          | 51/51245 [00:00<05:04, 167.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "[Conversion: Examples > Features]: 100%|██████████| 51245/51245 [06:02<00:00, 141.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace()\n",
    "args.model_name = \"Bert\"\n",
    "args.pretrained_name_or_path = \"dsksd/bert-ko-small-minimal\"\n",
    "args.model_dir = \"../models\"\n",
    "args.model_fold = \"trade-kfold\"\n",
    "args.data_dir = \"../input/data/train_dataset\"\n",
    "\n",
    "tokenizer, processor, slot_meta, tokenized_slot_meta, train_features, train_labels = get_informations(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_kfold_or_full(args, tokenizer, processor, slot_meta, tokenized_slot_meta, features, labels):\n",
    "    domain_group = {\n",
    "        '관광_식당':0,\n",
    "        '관광':1,\n",
    "        '지하철':2,\n",
    "        '택시':3,\n",
    "        '식당_택시':4,\n",
    "        '숙소_택시':5,\n",
    "        '식당':6,\n",
    "        '숙소_식당':7,\n",
    "        '숙소':8,\n",
    "        '관광_택시':9,\n",
    "        '관광_숙소_식당':10,\n",
    "        '관광_숙소':11,\n",
    "        '숙소_식당_택시':12,\n",
    "        '관광_식당_택시':13,\n",
    "        '관광_숙소_택시':14\n",
    "    }\n",
    "\n",
    "    features = np.array(features)\n",
    "    dialogue_features, dialogue_labels, domain_labels = defaultdict(list), defaultdict(list), []\n",
    "    for f in features:\n",
    "        dialogue = '-'.join(f.guid.split('-')[:-1])\n",
    "        # TODO: 의심스럽... 순서대로 들어갈까..? 순서대로가 아니어도 괜찮기는 함.. 흠\n",
    "        # dialogue_features는 키 값이 dialogue_idx이고 value가 feature들의 리스트로 이루어짐\n",
    "        dialogue_features[dialogue].append(f)\n",
    "\n",
    "    for k, v in dialogue_features.items():\n",
    "        feature_domain = '_'.join(sorted(v[0].domain))\n",
    "        if '지하철' in feature_domain:\n",
    "            feature_domain = '지하철'\n",
    "        domain_labels.append(domain_group[feature_domain])\n",
    "\n",
    "    for k, v in labels.items():\n",
    "        dialogue_labels['-'.join(k.split('-')[:-1])].append([k, v])\n",
    "\n",
    "    if args.isKfold:\n",
    "        kf = StratifiedKFold(n_splits=args.fold_num, random_state=args.seed, shuffle=True)\n",
    "        fold_idx = 1\n",
    "        \n",
    "        print(len(features))\n",
    "        print(len(domain_labels))\n",
    "\n",
    "        for train_index, dev_index in kf.split(list(dialogue_features.keys()), domain_labels):\n",
    "            # 찾은듯!! domain_labels는 길이가 domain 갯수 만큼이고, features는 turn 별 값일 것이다\n",
    "            # TODO: 여기서 출력 해보고 확인한자 - 갯수 달랐음\n",
    "            # os.makedirs(f'{args.model_dir}/{args.model_fold}/{fold_idx}-fold', exist_ok=True)\n",
    "\n",
    "            train_dialogue_features, dev_dialogue_features = np.array(list(dialogue_features.items()))[train_index.astype(int)], np.array(list(dialogue_features.items()))[dev_index.astype(int)]\n",
    "            \n",
    "            train_features, dev_features = [], []\n",
    "            [train_features.extend(t[1]) for t in train_dialogue_features]\n",
    "            [dev_features.extend(t[1]) for t in dev_dialogue_features]\n",
    "\n",
    "            dev_dialogue_labels = {k: labels[k] for k in dev_features}\n",
    "            dev_dialogue_labels = np.array(list(dialogue_labels.items()))[dev_index.astype(int)]\n",
    "            dev_labels = {t[0]:t[1] for turn in dev_dialogue_labels[:, 1] for t in turn}\n",
    "\n",
    "            train_loader = get_data_loader(processor, train_features, args.train_batch_size)\n",
    "            dev_loader = get_data_loader(processor, dev_features, args.eval_batch_size)\n",
    "\n",
    "            print(f\"========= {fold_idx} fold =========\")\n",
    "            train_model(args, tokenizer, processor, slot_meta, tokenized_slot_meta, fold_idx, train_loader, dev_loader, dev_labels)\n",
    "            fold_idx += 1\n",
    "        \n",
    "    else:\n",
    "        fold_idx = None\n",
    "        train_index, dev_index = train_test_split(np.array(range(len(dialogue_features))), test_size=0.1, random_state=args.seed, stratify=domain_labels)\n",
    "        \n",
    "        train_dialogue_features, dev_dialogue_features = np.array(list(dialogue_features.items()))[train_index.astype(int)], np.array(list(dialogue_features.items()))[dev_index.astype(int)]\n",
    "\n",
    "        train_features, dev_features = [], []\n",
    "        [train_features.extend(t[1]) for t in train_dialogue_features]\n",
    "        [dev_features.extend(t[1]) for t in dev_dialogue_features]\n",
    "\n",
    "        dev_dialogue_labels = np.array(list(dialogue_labels.items()))[dev_index.astype(int)]\n",
    "        dev_labels = {t[0]:t[1] for turn in dev_dialogue_labels[:, 1] for t in turn}\n",
    "\n",
    "        train_loader = get_data_loader(processor, train_features, args.train_batch_size)\n",
    "        dev_loader = get_data_loader(processor, dev_features, args.eval_batch_size)\n",
    "\n",
    "        train_model(args, tokenizer, processor, slot_meta, tokenized_slot_meta, fold_idx, train_loader, dev_loader, dev_labels)\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "51245\n7000\n"
     ]
    }
   ],
   "source": [
    "args.isKfold = True\n",
    "args.fold_num = 5\n",
    "args.seed = 42\n",
    "select_kfold_or_full(args, tokenizer, processor, slot_meta, tokenized_slot_meta, train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_features, dialogue_labels, domain_labels = defaultdict(list), defaultdict(list), []\n",
    "for f in train_features:\n",
    "    dialogue = '-'.join(f.guid.split('-')[:-1])\n",
    "    # TODO: 의심스럽... 순서대로 들어갈까..? 순서대로가 아니어도 괜찮기는 함.. 흠\n",
    "    # dialogue_features는 키 값이 dialogue_idx이고 value가 feature들의 리스트로 이루어짐\n",
    "    dialogue_features[dialogue].append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(dialogue_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'snowy-hat-8324:관광_식당_11'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "list(dialogue_features.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'snowy-hat-8324:관광_식당_11-3'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "dialogue_features['snowy-hat-8324:관광_식당_11'][3].guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(train_features)\n",
    "labels = train_labels\n",
    "\n",
    "domain_group = {\n",
    "        '관광_식당':0,\n",
    "        '관광':1,\n",
    "        '지하철':2,\n",
    "        '택시':3,\n",
    "        '식당_택시':4,\n",
    "        '숙소_택시':5,\n",
    "        '식당':6,\n",
    "        '숙소_식당':7,\n",
    "        '숙소':8,\n",
    "        '관광_택시':9,\n",
    "        '관광_숙소_식당':10,\n",
    "        '관광_숙소':11,\n",
    "        '숙소_식당_택시':12,\n",
    "        '관광_식당_택시':13,\n",
    "        '관광_숙소_택시':14\n",
    "    }\n",
    "dialogue_features, dialogue_labels, domain_labels = defaultdict(list), defaultdict(list), []\n",
    "for f in features:\n",
    "    dialogue = '-'.join(f.guid.split('-')[:-1])\n",
    "    dialogue_features[dialogue].append(f)\n",
    "\n",
    "for k, v in dialogue_features.items():\n",
    "    feature_domain = '_'.join(sorted(v[0].domain))\n",
    "    if '지하철' in feature_domain:\n",
    "        feature_domain = '지하철'\n",
    "    domain_labels.append(domain_group[feature_domain])\n",
    "\n",
    "for k, v in labels.items():\n",
    "    dialogue_labels['-'.join(k.split('-')[:-1])].append([k, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [51245, 7000]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-346c07545cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfold_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \"\"\"\n\u001b[1;32m    355\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 320\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [51245, 7000]"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    kf = StratifiedKFold(n_splits=args.fold_num, random_state=args.seed, shuffle=True)\n",
    "    fold_idx = 1\n",
    "    \n",
    "    for train_index, dev_index in kf.split(list(features), domain_labels):\n",
    "        print(train_index, dev_index)\n",
    "        print(len(train_index), len(dev_index))\n",
    "        # os.makedirs(f'{args.model_dir}/{args.model_fold}/{fold_idx}-fold', exist_ok=True)\n",
    "\n",
    "        # train_dialogue_keys, dev_dialogue_keys = np.array(list(dialogue_features.keys()))[train_index.astype(int)], np.array(list(dialogue_features.keys()))[dev_index.astype(int)]\n",
    "            \n",
    "        # train_features, dev_features = [], []\n",
    "        # [train_features.extend(dialogue_features[dialogue_id]) for dialogue_id in train_dialogue_keys]\n",
    "        # [dev_features.extend(dialogue_features[dialogue_id]) for dialogue_id in dev_dialogue_keys]\n",
    "\n",
    "        # dev_labels = {f.guid: labels[f.guid] for f in dev_features}\n",
    "        # train_loader = get_data_loader(processor, train_features, args.train_batch_size)\n",
    "        # dev_loader = get_data_loader(processor, dev_features, args.eval_batch_size)\n",
    " \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40952\n10293\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "51245\n"
     ]
    }
   ],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dialogue_labels_2 = np.array(list(dialogue_labels.items()))[dev_index.astype(int)]\n",
    "dev_labels_2 = {t[0]:t[1] for turn in dev_dialogue_labels[:, 1] for t in turn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OpenVocabDSTFeature(guid='summer-voice-4986:식당_관광_9-0', domain=['식당', '관광'], input_id=[2, 3, 10238, 27672, 4234, 15532, 4403, 4292, 3430, 4219, 5158, 7933, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3], [21832, 11764, 3]])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "dev_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_labels_3 = {f.guid: labels[f.guid] for f in dev_features}\n",
    "# labels['summer-voice-4986:식당_관광_9-0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dev_labels_3.items():\n",
    "    if dev_labels_2[k] != v:\n",
    "        print(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}